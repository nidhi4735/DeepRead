# ============================================
# Document Question-Answering System using LangChain + HuggingFace
# Author: Anushka
# ============================================

# --- Install required packages (run in Colab)
!pip install -q langchain langchain-community langchain-huggingface chromadb sentence-transformers pypdf

# --- Standard imports
import os
from dotenv import load_dotenv

# --- Load environment variables
load_dotenv()
HF_TOKEN = "..." #mention your HF_API token here

# --- Colab file upload utility
from google.colab import files

# --- LangChain imports
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate

# ============================================
# 1. Upload and load the document
# ============================================
uploaded = files.upload()
fname = list(uploaded.keys())[0]

# Load PDF or text file depending on file extension
loader = PyPDFLoader(fname) if fname.lower().endswith(".pdf") else TextLoader(fname, encoding="utf-8")
docs = loader.load()

# ============================================
# 2. Split document into smaller chunks
# ============================================
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(docs)

# ============================================
# 3. Build vector store using Chroma + HuggingFace embeddings
# ============================================
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = Chroma.from_documents(chunks, embeddings, persist_directory="chroma_db")

# Optimized retriever using Maximal Marginal Relevance (MMR)
retriever = db.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 3,          # Number of final documents to return
        "fetch_k": 10,   # Number of top docs to consider before reranking
        "lambda_mult": 0.7  # Balance between relevance and diversity
    }
)

# ============================================
# 4. LLM Setup using HuggingFace Endpoint
# ============================================
llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.3",
    task="text2text-generation",
    huggingfacehub_api_token=HF_TOKEN,
    temperature=0.0,
    max_new_tokens=512
)

# Wrap LLM for chat-style interaction
model = ChatHuggingFace(llm=llm)

# ============================================
# 5. Define strict prompt template
# ============================================
template_text = """You are a precise and strict assistant.
Answer the question using ONLY the information provided in the context.
Do NOT include any assumptions, opinions, or external knowledge.
Keep your answer concise, clear, and strictly relevant.
Limit your response to 150-300 words. Use proper spacing and formatting.
ALWAYS CHECK AND VERIFY YOUR ANSWER BEFORE ANSWERING.

Question: {question}

Context:
{context}

Answer:"""

prompt = PromptTemplate(template=template_text, input_variables=["question", "context"])

# ============================================
# 6. Function to answer a user query
# ============================================
def answer_question(q: str) -> str:
    # Retrieve relevant document chunks using MMR retriever
    docs = retriever.get_relevant_documents(q)
    
    # Combine the content of the retrieved documents
    context = "\n\n".join([d.page_content for d in docs])
    
    # Format the prompt with question and context
    formatted_prompt = prompt.format(question=q, context=context)
    
    # Invoke the LLM
    response = model.invoke(formatted_prompt)
    
    return response.content.strip()

# ============================================
# 7. Interactive query loop
# ============================================
print("Ready. Type 'exit' or 'quit' to stop.")
while True:
    q = input("Ask: ")
    if q.lower() in ("exit", "quit"):
        break
    print(answer_question(q))
