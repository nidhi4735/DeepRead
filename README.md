# DeepRead
### Document Question-Answering System with LangChain & HuggingFace

A Python-based interactive system that allows users to ask questions over uploaded PDF or text documents.  
The system leverages **LangChain**, **Chroma Vector Database**, and **HuggingFace LLMs** to deliver precise, context-aware answers.

---

## üöÄ Features

- **Supports PDF and Text files** ‚Äì Upload and process PDF or text documents.
- **Document Chunking** ‚Äì Splits large documents into smaller, retrievable chunks.
- **Vector Search (MMR)** ‚Äì Uses **Chroma** with *Maximal Marginal Relevance* for relevant chunk retrieval.
- **Contextual QA** ‚Äì Answers based strictly on document context.
- **HuggingFace LLM Integration** ‚Äì Ensures natural, high-quality text generation.
- **Interactive Query Loop** ‚Äì Enables continuous question‚Äìanswering in one session.

---

## ‚öôÔ∏è Installation

Designed for **Google Colab** or local Python 3.10+ environments.

```bash
# Install dependencies
pip install -q langchain langchain-community langchain-huggingface chromadb sentence-transformers pypdf
```
## ‚úÖUsage

- 1Ô∏è‚É£ Run DeepRead in a Python environment or Colab cell
- 2Ô∏è‚É£ Upload your document (PDF or TXT)
- 3Ô∏è‚É£ Ask context-based questions interactively

# Example Interaction:
- Upload a document: sample.pdf
- Question: What is the key finding of section 3?
- Answer: The model achieved state-of-the-art accuracy using transfer learning.

## üß† How It Works

- Document Upload ‚Äì User uploads a .pdf or .txt file.
- Text Extraction & Chunking ‚Äì The file is split into semantically meaningful chunks.
- Vector Embedding ‚Äì Each chunk is embedded using a Sentence Transformer model.
- Storage ‚Äì Embeddings are stored in ChromaDB for fast retrieval.
- Query Handling ‚Äì User questions are embedded and compared to stored chunks.
- Answer Generation ‚Äì The top results are passed to a HuggingFace LLM via LangChain to produce a contextual answer
